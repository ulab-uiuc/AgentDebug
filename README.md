# AgentDebug ğŸ”ğŸ¤–

<div align="center">
  <img src="assets/logo.png" alt="AgentDebug Logo" width="400"/>

  **Where LLM Agents Fail and How They Can Learn From Failures**

  [![Paper](https://img.shields.io/badge/Paper-arXiv-b31b1b.svg)](https://arxiv.org/abs/2509.25370)
  [![Dataset](https://img.shields.io/badge/Dataset-AgentErrorBench-blue.svg)](https://drive.google.com/drive/folders/1bQe6dQA85pktT63YnKIKJDTVaH3O3Vpu?usp=drive_link)
  [![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
</div>

## ğŸ“Š AgentErrorBench Dataset

Access our comprehensive benchmark dataset of systematically annotated failure trajectories:

ğŸ”— **[Download AgentErrorBench](https://drive.google.com/drive/folders/1bQe6dQA85pktT63YnKIKJDTVaH3O3Vpu?usp=drive_link)**

AgentErrorBench contains 200 expertly annotated agent failure trajectories across three environments:
- **GAIA**: 50 trajectories from general AI assistant tasks
- **ALFWorld**: 100 trajectories from embodied agent tasks
- **WebShop**: 50 trajectories from web navigation and shopping tasks

## ğŸ“– About

Large Language Model (LLM) agents have shown remarkable capabilities in solving complex, multi-step tasks through sophisticated architectures integrating planning, memory, reflection, and tool-use modules. However, these complex systems are vulnerable to cascading failures, where a single root-cause error propagates through subsequent decisions, ultimately leading to task failure.

**AgentDebug** introduces a principled framework for understanding, detecting, and recovering from agent failures through three key contributions:

1. **AgentErrorTaxonomy** ğŸ“‹: A modular classification system categorizing failure modes across memory, reflection, planning, action, and system-level operations.

2. **AgentErrorBench** ğŸ¯: The first comprehensive dataset of systematically annotated failure trajectories from real-world agent rollouts in ALFWorld, GAIA, and WebShop environments.

3. **AgentDebug Framework** ğŸ› ï¸: An intelligent debugging system that isolates root-cause failures and provides targeted corrective feedback, enabling agents to recover and iteratively improve.

## ğŸš€ Key Results

Our experiments demonstrate that AgentDebug significantly improves agent reliability:

- **24% higher** all-correct accuracy compared to the strongest baseline
- **17% higher** step accuracy in error detection
- **Up to 26%** relative improvement in task success rates through iterative recovery
- Effective across diverse environments (ALFWorld, GAIA, WebShop)

## ğŸ—ï¸ Architecture

The AgentDebug framework consists of a two-stage analysis pipeline:

### Stage 1: Fine-Grained Analysis
Performs detailed step-by-step analysis of agent trajectories to identify potential error patterns at each decision point.

### Stage 2: Critical Error Detection
Identifies the critical failure point that led to task failure and provides root cause analysis with targeted feedback.

## ğŸ“ Repository Structure

```
AgentDebug/
â”œâ”€â”€ detector/              # Core detection and analysis framework
â”‚   â”œâ”€â”€ fine_grained_analysis.py    # Stage 1: Step-level error detection
â”‚   â”œâ”€â”€ critical_error_detection.py # Stage 2: Critical failure identification
â”‚   â”œâ”€â”€ error_definitions.py        # Comprehensive error taxonomy
â”‚   â””â”€â”€ AgentErrorBench/           # Benchmark dataset structure
â”‚       â”œâ”€â”€ Label/                  # Annotated error labels
â”‚       â””â”€â”€ Original_Failure_Trajectory/ # Raw agent trajectories
â””â”€â”€ README.md
```

## ğŸ”§ Installation

```bash
git clone https://github.com/ulab-uiuc/AgentDebug.git
cd AgentDebug
pip install -r requirements.txt
```

## ğŸ’¡ Quick Start

```python
from detector import fine_grained_analysis, critical_error_detection

# Load your agent trajectory
trajectory = load_agent_trajectory("path/to/trajectory.json")

# Stage 1: Analyze potential errors at each step
step_errors = fine_grained_analysis.analyze(trajectory)

# Stage 2: Identify critical failure point
critical_failure = critical_error_detection.detect(trajectory, step_errors)

# Get corrective feedback
feedback = critical_failure.generate_feedback()
```

## ğŸ“Š Error Taxonomy

Our comprehensive error taxonomy covers five key modules:

| Module | Error Types | Examples |
|--------|------------|----------|
| **Memory** | Hallucination, Retrieval Failure, Over-simplification | Agent forgets visited locations |
| **Reflection** | Progress Misjudgment, Outcome Misinterpretation | Incorrect assessment of task progress |
| **Planning** | Inefficient Planning, Constraint Ignorance | Selecting impossible actions |
| **Action** | Format Errors, Parameter Errors, Misalignment | Invalid action syntax |
| **System** | Step Limits, Tool Failures, Environment Errors | Exceeding maximum steps |

## ğŸ“ˆ Performance

AgentDebug achieves state-of-the-art performance in error detection and recovery:

| Metric | Improvement |
|--------|------------|
| All-Correct Accuracy | +24% |
| Step Accuracy | +17% |
| Task Success Rate | Up to +26% |

## ğŸ“ Citation

If you use AgentDebug in your research, please cite our paper:

```bibtex
@article{agentdebug2024,
  title={Where LLM Agents Fail and How They Can Learn From Failures},
  author={Author Names},
  journal={arXiv preprint arXiv:2509.25370},
  year={2024}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

We welcome contributions! Please feel free to submit issues, create pull requests, or reach out for collaborations.

## ğŸ“§ Contact

For questions and support, please open an issue on this repository.

---

<div align="center">
  <sub>Built with â¤ï¸ by the ULAB team at UIUC</sub>
</div>